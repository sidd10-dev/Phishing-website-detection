# Chunk 1
library(tidyverse)
library(lattice)
library(reshape2)
library(pipeR)
library(rlist)
# Chunk 2
df <- read.csv("../datasets/cleaned_dataset.csv")
# Chunk 1
library(tidyverse)
library(lattice)
library(reshape2)
library(pipeR)
library(rlist)
# Chunk 2
df <- read.csv("D:/Projects/Phishing website detection/datasets/cleaned_dataset.csv")
df %>% head()
# Chunk 3
df %>% nrow()
df %>% ncol()
# Chunk 4
corr.mat <- round(cor(df), 2)
melted.corr.mat <- melt(corr.mat)
melted.corr.mat %>%
ggplot(aes(x=Var1, y=Var2, fill=value)) +
geom_tile()
# Chunk 5
corr.mat[!lower.tri(corr.mat)] <- NA
cols <- data.frame(corr.mat) %>%
rownames_to_column() %>%
gather(key="variable", value="correlation", -rowname) %>%
filter(abs(correlation) > 0.2 & rowname == "status") %>%
arrange(correlation) %>%
pull(variable)
cols
# Chunk 6
df.filtered <- df %>%
select(cols, status) %>%
mutate(status = as.factor(status))
df.filtered %>% head()
# Chunk 7
df.filtered %>%
ggplot(aes(y=page_rank, x=status, fill=status)) +
geom_boxplot()
# Chunk 8
df.filtered %>%
ggplot(aes(x=status, fill=as.factor(nb_www))) +
geom_bar(position="dodge", colour = "black") +
scale_fill_viridis_d()
# Chunk 9
df.filtered %>%
ggplot(aes(x=status, y=nb_hyperlinks, fill=status)) +
geom_boxplot()
# Chunk 10
df.filtered %>%
arrange(nb_hyperlinks) %>%
ggplot() +
geom_histogram(aes(x=nb_hyperlinks), bins=100, fill="deepskyblue3", color="black")
# Chunk 11
df.filtered %>%
arrange(nb_hyperlinks) %>%
ggplot() +
geom_histogram(aes(x=domain_age), bins=100, fill="deepskyblue3", color="black")
# Chunk 12
df.filtered %>%
ggplot(aes(x=status, y=domain_age, fill=status)) +
geom_violin()
# Chunk 13
summary(df$ratio_intHyperlinks)
# Chunk 14
df.filtered %>%
arrange(nb_hyperlinks) %>%
ggplot(aes(x=ratio_intHyperlinks, fill=status)) +
geom_density(bins=100, color="black", alpha=0.3)
# Chunk 15
summary(df$nb_dots)
# Chunk 16
df.filtered %>%
ggplot(aes(y=nb_dots)) +
geom_boxplot(fill="deepskyblue3")
# Chunk 17
df.filtered %>%
ggplot(aes(y=nb_dots, x=status, fill=status)) +
geom_boxplot()
# Chunk 18
summary(df$tld_in_subdomain)
# Chunk 19
df.filtered %>%
ggplot(aes(y=tld_in_subdomain)) +
geom_boxplot(fill="deepskyblue3")
# Chunk 20
df.filtered %>%
mutate(tld = as.factor(tld_in_subdomain)) %>%
ggplot(aes(x=tld, fill=tld)) +
geom_bar(colour="black") +
scale_fill_viridis_d() +
ggtitle("TLD_IN_SUBDOMAIN DISTRIBUTION")
# Chunk 21
df.filtered %>%
mutate(tld = as.factor(tld_in_subdomain)) %>%
ggplot(aes(x=tld, fill=status)) +
geom_bar(colour="black", position="dodge") +
scale_fill_viridis_d() +
ggtitle("TLD_IN_SUBDOMAIN distribution with status hue")
# Chunk 22
summary(df$prefix_suffix)
# Chunk 23
df.filtered %>%
mutate(ps = as.factor(prefix_suffix)) %>%
ggplot(aes(x=ps, fill=ps)) +
geom_bar(colour="black") +
scale_fill_viridis_d() +
ggtitle("Prefix_Suffix DISTRIBUTION")
# Chunk 24
df.filtered %>%
mutate(ps = as.factor(prefix_suffix)) %>%
ggplot(aes(x=ps, fill=status)) +
geom_bar(colour="black", position="dodge") +
scale_fill_viridis_d() +
ggtitle("Prefix_Suffix DISTRIBUTION with status hue")
# Chunk 25
summary(df$longest_word_path)
# Chunk 26
df.filtered %>%
ggplot(aes(y=longest_word_path)) +
geom_boxplot() +
ggtitle("Longest word path distribution")
# Chunk 27
df.filtered %>%
ggplot(aes(y=longest_word_path, x=as.factor(status), fill=status)) +
geom_boxplot() +
ggtitle("Longest word path distribution")
# Chunk 28
summary(df$empty_title)
# Chunk 29
df.filtered %>%
mutate(et = as.factor(empty_title)) %>%
ggplot(aes(x=et, fill=et)) +
geom_bar(colour="black") +
scale_fill_viridis_d() +
ggtitle("Empty title distribution")
# Chunk 30
df.filtered %>%
mutate(et = as.factor(empty_title)) %>%
ggplot(aes(x=et, fill=status)) +
geom_bar(colour="black", position="dodge") +
scale_fill_viridis_d() +
ggtitle("Empty title distribution with status hue")
df.filtered %>%
ggplot(aes(y=nb_dots)) +
geom_boxplot(fill="deepskyblue3") +
ggtitle("Number of dots in URL")
df.filtered %>%
ggplot(aes(y=nb_dots, x=status, fill=status)) +
geom_boxplot() +
ggtitle("Number of dots in URL")
df.filtered %>%
arrange(nb_hyperlinks) %>%
ggplot(aes(x=ratio_intHyperlinks, fill=status)) +
geom_density(bins=100, color="black", alpha=0.3) +
ggtitle("Number of Hyperlinks in website")
df.filtered %>%
ggplot(aes(x=status, y=domain_age, fill=status)) +
geom_violin() +
ggtitle("Domain Age")
df.filtered %>%
ggplot(aes(x=status, y=nb_hyperlinks, fill=status)) +
geom_boxplot() +
ggtitle("Box plot of nb_hyperlinks")
df.filtered %>%
ggplot(aes(x=status, fill=as.factor(nb_www))) +
geom_bar(position="dodge", colour = "black") +
scale_fill_viridis_d()
df.filtered %>%
ggplot(aes(x=status, fill=as.factor(nb_www))) +
geom_bar(position="dodge", colour = "black") +
scale_fill_viridis_d() +
ggtitle("Number of www in URL")
df.filtered %>%
ggplot(aes(y=page_rank, x=status, fill=status)) +
geom_boxplot() +
ggtitle("Page Rank vs Status")
cols <- data.frame(corr.mat) %>%
rownames_to_column() %>%
gather(key="variable", value="correlation", -rowname) %>%
filter(abs(correlation) > 0.2 & rowname == "status") %>%
arrange(correlation) %>%
pull(variable)
corr.mat <- round(cor(df), 2)
melted.corr.mat <- melt(corr.mat)
melted.corr.mat %>%
ggplot(aes(x=Var1, y=Var2, fill=value)) +
geom_tile()
corr.mat[!lower.tri(corr.mat)] <- NA
cols <- data.frame(corr.mat) %>%
rownames_to_column() %>%
gather(key="variable", value="correlation", -rowname) %>%
filter(abs(correlation) > 0.2 & rowname == "status") %>%
arrange(correlation) %>%
pull(variable)
cols
corr.mat <- round(cor(df), 2)
melted.corr.mat <- melt(corr.mat)
melted.corr.mat %>%
ggplot(aes(x=Var1, y=Var2, fill=value)) +
geom_tile +
ggtitle("Correlation Heatmap")
corr.mat <- round(cor(df), 2)
melted.corr.mat <- melt(corr.mat)
melted.corr.mat %>%
ggplot(aes(x=Var1, y=Var2, fill=value)) +
geom_tile() +
ggtitle("Correlation Heatmap")
# Chunk 1
library(tidyverse)
library(e1071)
# Chunk 2
train <- read.csv("D:/Projects/Phishing website detection/datasets/train.csv")
test <- read.csv("D:/Projects/Phishing website detection/datasets/test.csv")
# Chunk 3
model <- svm(formula = status ~ ., data = train)
model
# Chunk 4
preds <- predict(model, test)
preds <- ifelse(preds > 0.5, 1, 0)
cm <- table(preds, test %>% pull('status'))
(cm[1] + cm[4])/nrow(test)
# Chunk 5
library('e1071')
library('parsnip')
library('ranger')
library('tidyverse')
library('pso')
# Chunk 6
#PSO Optimisation
#Defining fitness function
fit_function <- function(x){
a <- x[1]
b <- x[2]
#define model
model <- svm(formula = status ~ .,
data = train,
kernel = 'radial',
type = 'eps-regression',
cost = a,
gamma = b)
preds <- predict(model, test)
preds <- ifelse(preds > 0.5, 1, 0)
cm <- table(preds, test %>% pull('status'))
acc <- (cm[1] + cm[4])/nrow(test)
return(-acc)
}
# Chunk 7
pso <- psoptim(par = rep(NA,2), fn = fit_function, lower = c(0.1,0.001), upper = c(2, 0.1),
control = list(maxit = 1, vectorize = T, s = 1))
pso$par
# Chunk 8
model <- svm(formula = status ~ .,
data = train,
kernel = 'radial',
type = 'eps-regression',
cost = pso$par[1],
gamma = pso$par[2])
preds <- predict(model, test)
preds <- ifelse(preds > 0.5, 1, 0)
cm <- table(preds, test %>% pull('status'))
acc <- (cm[1] + cm[4])/nrow(test)
print(acc)
#PSO Optimisation
#Defining fitness function
fit_function <- function(x){
a <- x[1]
b <- x[2]
#define model
model <- svm(formula = status ~ .,
data = train,
kernel = 'radial',
type = 'eps-regression',
cost = a,
gamma = b)
preds <- predict(model, test)
preds <- ifelse(preds > 0.5, 1, 0)
cm <- table(preds, test %>% pull('status'))
acc <- (cm[1] + cm[4])/nrow(test)
return(-acc)
}
pso <- psoptim(par = rep(NA,2), fn = fit_function, lower = c(0.1,0.001), upper = c(2, 0.1),
control = list(maxit = 10, vectorize = T, s = 5))
pso$par
model <- svm(formula = status ~ .,
data = train,
kernel = 'radial',
type = 'eps-regression',
cost = pso$par[1],
gamma = pso$par[2])
preds <- predict(model, test)
preds <- ifelse(preds > 0.5, 1, 0)
cm <- table(preds, test %>% pull('status'))
acc <- (cm[1] + cm[4])/nrow(test)
print(acc)
library(tidyverse)
library(randomForest)
train <- read.csv("D:/Projects/Phishing website detection/datasets/train.csv")
test <- read.csv("D:/Projects/Phishing website detection/datasets/test.csv")
train <- train %>%
mutate(status = as.factor(status))
test <- test %>%
mutate(status = as.factor(status))
model.rf <- randomForest(x = train %>% select(-status), y = train$status, ntree=400)
predictions <- predict(model.rf, test)
cm <- table(predictions, test$status)
accuracy <- (cm[1] + cm[4])/nrow(test) * 100
accuracy
library('e1071')
library('parsnip')
library('ranger')
library('tidyverse')
library('pso')
#PSO Optimisation
#Defining fitness function
fit_function <- function(x){
a <- as.integer(x[1])
#define model
model <- randomForest(x = train %>% select(-status), y = train$status, ntree=a)
preds <- predict(model, test)
cm <- table(preds, test %>% pull('status'))
acc <- (cm[1] + cm[4])/nrow(test)
return(-acc)
}
pso <- psoptim(par = rep(NA,1), fn = fit_function, lower = c(10), upper = c(500),
control = list(maxit = 10, vectorize = T, s = 2))
pso$par
a <- as.integer(pso$par[1])
#define model
model <- randomForest(x = train %>% select(-status), y = train$status, ntree=a)
preds <- predict(model, test)
cm <- table(preds, test %>% pull('status'))
acc <- (cm[1] + cm[4])/nrow(test)
acc
pso <- psoptim(par = rep(NA,1), fn = fit_function, lower = c(10), upper = c(500),
control = list(maxit = 20, vectorize = T, s = 2))
pso$par
a <- as.integer(pso$par[1])
#define model
model <- randomForest(x = train %>% select(-status), y = train$status, ntree=a)
preds <- predict(model, test)
cm <- table(preds, test %>% pull('status'))
acc <- (cm[1] + cm[4])/nrow(test)
acc
model.rf <- randomForest(x = train %>% select(-status), y = train$status)
predictions <- predict(model.rf, test)
cm <- table(predictions, test$status)
accuracy <- (cm[1] + cm[4])/nrow(test) * 100
accuracy
acc
model.rf <- randomForest(x = train %>% select(-status), y = train$status, ntree = 100)
predictions <- predict(model.rf, test)
cm <- table(predictions, test$status)
accuracy <- (cm[1] + cm[4])/nrow(test) * 100
accuracy
model.rf <- randomForest(x = train %>% select(-status), y = train$status, ntree = 100)
predictions <- predict(model.rf, test)
cm <- table(predictions, test$status)
accuracy <- (cm[1] + cm[4])/nrow(test) * 100
accuracy
a <- as.integer(pso$par[1])
#define model
model <- randomForest(x = train %>% select(-status), y = train$status, ntree=a)
preds <- predict(model, test)
cm <- table(preds, test %>% pull('status'))
acc <- (cm[1] + cm[4])/nrow(test)
acc
a
library(tidyverse)
library(e1071)
library(caTools)
library(class)
train <- read.csv("D:/Projects/Phishing website detection/datasets/train.csv")
test <- read.csv("D:/Projects/Phishing website detection/datasets/test.csv")
train <- train %>%
mutate(status = as.factor(status))
test <- test %>%
mutate(status = as.factor(status))
model.knn <- knn(train=train, test=test, cl=train$status, k = 3)
predictions <- predict(model.knn, test)
cm <- table(test_cl$Species, model.knn)
cm <- table(test$status, model.knn)
accuracy <- (cm[1] + cm[4])/nrow(test) * 100
accuracy
#PSO Optimisation
#Defining fitness function
fit_function <- function(x){
a <- as.integer(x[1])
#define model
model <- knn(train=train, test=test, cl=train$status, k = a)
cm <- table(test$status, model)
accuracy <- (cm[1] + cm[4])/nrow(test)
return(-acc)
}
pso <- psoptim(par = rep(NA,1), fn = fit_function, lower = c(1), upper = c(20),
control = list(maxit = 20, vectorize = T, s = 2))
pso$par
a <- as.integer(pso$par[1])
#define model
model <- knn(train=train, test=test, cl=train$status, k = a)
cm <- table(test$status, model)
acc <- (cm[1] + cm[4])/nrow(test)
acc
library(tidyverse)
library(e1071)
library(caTools)
library(class)
train <- read.csv("D:/Projects/Phishing website detection/datasets/train.csv")
test <- read.csv("D:/Projects/Phishing website detection/datasets/test.csv")
train <- train %>%
mutate(status = as.factor(status))
test <- test %>%
mutate(status = as.factor(status))
model.knn <- knn(train=train, test=test, cl=train$status, k = 3)
cm <- table(test$status, model.knn)
accuracy <- (cm[1] + cm[4])/nrow(test) * 100
accuracy
library('e1071')
library('parsnip')
library('ranger')
library('tidyverse')
library('pso')
#PSO Optimisation
#Defining fitness function
fit_function <- function(x){
a <- as.integer(x[1])
#define model
model <- knn(train=train, test=test, cl=train$status, k = a)
cm <- table(test$status, model)
accuracy <- (cm[1] + cm[4])/nrow(test)
return(-accuracy)
}
pso <- psoptim(par = rep(NA,1), fn = fit_function, lower = c(1), upper = c(20),
control = list(maxit = 20, vectorize = T, s = 2))
pso$par
a <- as.integer(pso$par[1])
#define model
model <- knn(train=train, test=test, cl=train$status, k = a)
cm <- table(test$status, model)
acc <- (cm[1] + cm[4])/nrow(test)
acc
model <- svm(formula = status ~ .,
data = train,
kernel = 'radial',
type = 'eps-regression',
cost = 2,
gamma = 0.1)
library(tidyverse)
library(e1071)
train <- read.csv("D:/Projects/Phishing website detection/datasets/train.csv")
test <- read.csv("D:/Projects/Phishing website detection/datasets/test.csv")
model <- svm(formula = status ~ .,
data = train,
kernel = 'radial',
type = 'eps-regression',
cost = 2,
gamma = 0.1)
preds <- predict(model, test)
preds <- ifelse(preds > 0.5, 1, 0)
cm <- table(preds, test %>% pull('status'))
acc <- (cm[1] + cm[4])/nrow(test)
print(acc)
cm[1]/(cm[1] + cm[2])
cm[1]/(cm[1] + cm[3])
library(tidyverse)
library(randomForest)
train <- read.csv("D:/Projects/Phishing website detection/datasets/train.csv")
test <- read.csv("D:/Projects/Phishing website detection/datasets/test.csv")
train <- train %>%
mutate(status = as.factor(status))
test <- test %>%
mutate(status = as.factor(status))
model.rf <- randomForest(x = train %>% select(-status), y = train$status, ntree = 100)
predictions <- predict(model.rf, test)
cm <- table(predictions, test$status)
accuracy <- (cm[1] + cm[4])/nrow(test) * 100
accuracy
cm[1]/(cm[1] + cm[2])
cm[1]/(cm[1] + cm[3])
library('e1071')
library('parsnip')
library('ranger')
library('tidyverse')
library('pso')
#PSO Optimisation
#Defining fitness function
fit_function <- function(x){
a <- as.integer(x[1])
#define model
model <- randomForest(x = train %>% select(-status), y = train$status, ntree=a)
preds <- predict(model, test)
cm <- table(preds, test %>% pull('status'))
acc <- (cm[1] + cm[4])/nrow(test)
return(-acc)
}
pso <- psoptim(par = rep(NA,1), fn = fit_function, lower = c(10), upper = c(500),
control = list(maxit = 20, vectorize = T, s = 2))
pso$par
a <- as.integer(pso$par[1])
#define model
model <- randomForest(x = train %>% select(-status), y = train$status, ntree=a)
preds <- predict(model, test)
cm <- table(preds, test %>% pull('status'))
acc <- (cm[1] + cm[4])/nrow(test)
acc
cm[1]/(cm[1] + cm[2])
cm[1]/(cm[1] + cm[3])
